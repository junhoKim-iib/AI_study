{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OEXQV-kt8r4s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml   # Fetch dataset from openml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml(\"mnist_784\", version=1, cache=True, as_frame=False)# mnist 데이터 가져오기. 28 * 28 = 784개의 속성 "
      ],
      "metadata": {
        "id": "-h3Lnmrt86wK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습데이터, 타겟 데이터 \n",
        "X = mnist.data\n",
        "y = mnist.target"
      ],
      "metadata": {
        "id": "T-Eh8xhL86KL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch   # pytorch \n",
        "from torch.utils.data import TensorDataset, DataLoader  # data loading utility\n",
        "from sklearn.model_selection import train_test_split  # Split arrays or matrices into random train and test subsets"
      ],
      "metadata": {
        "id": "4z7b1cBnbQ0w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion of the dataset to include in the train split : 1/7 , random_state: None \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=1/7, random_state=0)\n",
        "# multi-dimensional matrix containing elements of a single data type\n",
        "X_train = torch.Tensor(X_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "# 64-bit integer (signed)\n",
        "y_train = torch.LongTensor(list(map(int, y_train)))\n",
        "y_test = torch.LongTensor(list(map(int, y_test)))"
      ],
      "metadata": {
        "id": "_K4TfUkjbW84"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn  # neural network \n",
        "import torch.nn.functional as F # dropout, relu \n",
        "from torch import optim # optimization algorithms\n",
        "from torch.autograd import Variable # automatic differentiation"
      ],
      "metadata": {
        "id": "-GFJVdzCbhvu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.view(-1, 1, 28,28).float() # reshape\n",
        "X_test = X_test.view(-1, 1, 28,28).float()  # reshape \n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iimEKM5ibp_u",
        "outputId": "b5cd83c0-dba3-40f0-ca2f-382a8db07c12"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 1, 28, 28])\n",
            "torch.Size([10000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = TensorDataset(X_train, y_train)  # training data\n",
        "test = TensorDataset(X_test, y_test)  # test data \n",
        "BATCH_SIZE = 16\n",
        "loader_train = DataLoader(train, batch_size= BATCH_SIZE, shuffle=False) # batch size = 16, no shuffle \n",
        "loader_test = DataLoader(test, batch_size= BATCH_SIZE, shuffle=False)  # batch size = 16, no shuffle \n"
      ],
      "metadata": {
        "id": "GIqau22nb2Y2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential( \n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss() # loss function. CrossEntropy\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.001) # adam optimizer. learning rate : 0.001\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "    "
      ],
      "metadata": {
        "id": "kVrI84uPcIuH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, loader_train):\n",
        "    optimizer = torch.optim.Adam(model.parameters()) # use Adam optimizer \n",
        "    error = nn.CrossEntropyLoss() # use Cross entropy \n",
        "    EPOCHS = 100 # 에포크 \n",
        "    model.train() # 모델 학습 \n",
        "    for epoch in range(EPOCHS): # EPOCHS 만큼 반복 \n",
        "        correct = 0 # 초기화 \n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(loader_train): # loader_train 순회 \n",
        "            var_X_batch = Variable(X_batch).float() # PyTorch Tensor의 Wrapper\n",
        "            var_y_batch = Variable(y_batch)  # PyTorch Tensor의 Wrapper\n",
        "            optimizer.zero_grad() # 그레디언트 초기화 \n",
        "            output = model(var_X_batch)  # 모델 적용 \n",
        "            loss = error(output, var_y_batch) # 손실\n",
        "            loss.backward() # 손실 역전파 \n",
        "            optimizer.step() # 역전파 단계에서 수집된 변화도로 매개변수를 조정\n",
        "            predicted = torch.max(output.data, 1)[1]   #  return max_indices\n",
        "            correct += (predicted == var_y_batch).sum() # correct count \n",
        "            if batch_idx % 200 == 0:\n",
        "                print('에포크 : {} [{}/{} ({:.0f}%)]\\t 손실함수 : {:.6f}\\t Accuracy:{:.3f}%'\\\n",
        "                      .format(epoch, batch_idx*len(X_batch),len(loader_train),\\\n",
        "                        100.*batch_idx / len(loader_train),loss.data,correct*100./ (BATCH_SIZE*(batch_idx + 1))))\n"
      ],
      "metadata": {
        "id": "dwXs-ffUwLnG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model): # 모델 평가 \n",
        "    correct = 0\n",
        "    for test_imgs, test_labels in loader_test:\n",
        "        test_imgs = Variable(test_imgs).float()\n",
        "        output = model(test_imgs)\n",
        "        # 가장 높은 값을 가진 인덱스:  예측값\n",
        "        predicted = torch.max(output,1)[1]\n",
        "        correct += (predicted == test_labels).sum()\n",
        "    print(\"테스트 데이터 정확도: {:.3f}% \".format(float(correct) / (len(loader_test)*BATCH_SIZE)))\n",
        "\n",
        "cnn = CNN()\n",
        "evaluate(cnn)\n",
        "fit(cnn, loader_train)\n",
        "cnn.eval()\n",
        "evaluate(cnn)\n",
        "index = 10\n",
        "data = X_test[index].view(-1, 1, 28, 28).float()\n",
        "output = cnn(data)\n",
        "print('{} 번째 학습데이터의 테스트 결과 : {}'.format(index,output))\n",
        "_, predicted = torch.max(output, 1)\n",
        "print('{} 번째 데이터의 예측측 : {}'.format(index, predicted.numpy()))\n",
        "print('실제 레이블 : {}'.format(y_test[index]))"
      ],
      "metadata": {
        "id": "U4FoSAdDx9_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZ5FtxPuy61U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}